{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa5cc36-3d59-4157-b820-3e28c67b6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import nlu\n",
    "import pandas as pd\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdc18d2-8016-41c6-8d37-d621cd5b4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/training_set.csv')\n",
    "df_test = pd.read_csv('data/testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31dda1c-c23e-41d0-9fe2-118dc89bb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict = []\n",
    "for i, report in enumerate(df_train[\"Findings\"]):\n",
    "    _, sentence = tokenize_report(report)\n",
    "    for sent in sentence:\n",
    "        if sent not in sentence_dict:\n",
    "            sentence_dict.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387d872-ecbb-457f-b693-cf1a9a529a2c",
   "metadata": {},
   "source": [
    "# Sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013fe0d-62a2-42cc-8cc0-f37507d8c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Model (Sbert)\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = sentence_dict\n",
    "sentences.remove('')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bf4e8-bb5e-4705-82ed-5fdbc961a268",
   "metadata": {},
   "source": [
    "# BioBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfb9de4-951d-4384-8b5f-2941fa182c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4344/3741706361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en.embed_sentence.biobert.pmc_base_cased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'I hate cancer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\nlu\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(request, path, verbose, gpu, streamlit_caching)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mis_authenticated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0mauth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# check if secets are in default loc, if yes load them and create licensed context automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_open_source_spark_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclearCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\nlu\\__init__.py\u001b[0m in \u001b[0;36mget_open_source_spark_context\u001b[1;34m(gpu)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_env_pyspark_2_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark23\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_env_pyspark_2_4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark24\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mis_env_pyspark_3_0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_env_pyspark_3_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Current Spark version {get_pyspark_version()} not supported!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\sparknlp\\__init__.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(gpu, spark23, spark24, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkRealTimeOutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mspark_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_without_realtime_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\sparknlp\\__init__.py\u001b[0m in \u001b[0;36mstart_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.jsl.settings.storage.cluster_tmp_dir\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_tmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_with_realtime_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\X-ray\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "model = nlu.load('en.embed_sentence.biobert.pmc_base_cased').predict(['I hate cancer'], output_level='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea8645f8-cb06-429a-bc82-dad89b28e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5401, 384) 5401\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa89a2da-15ff-46f5-b94b-456f49c9e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_embedding(embeddings, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ab32c-8b21-4da4-9cf3-92fa2d7bf7ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TMLCC Tensorboard Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc00332-97ac-4d67-8738-caf81c5defab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Latent_feat_last_hope_train.csv')\n",
    "df_num = pd.read_csv('clean_train.csv').set_index('MOFname')\n",
    "df_topo = pd.read_csv('train.csv').set_index('MOFname').topology\n",
    "y = df.drop(['target', 'mofname'], axis=1).to_numpy()\n",
    "label = df['target'].to_numpy()\n",
    "error = np.abs(df['target'].to_numpy() - df['predict'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb180e4f-e55a-410c-901d-88e8a76f971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_num, on='mofname')\n",
    "surface_area = df['surface_area [m^2/g]']\n",
    "df = df.join(df_topo, on='mofname')\n",
    "topo = df['topology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33dd4272-e7a0-4466-9040-beb8fd6623be",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {}\n",
    "metadata_dict['target'] = label\n",
    "metadata_dict['error'] = error\n",
    "metadata_dict['surface area'] = surface_area\n",
    "metadata_dict['topology'] = topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a3fb82-122b-468c-8ea5-3f2548dd2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volume [A^3]', 'weight [u]', 'density [g/cm^3]', 'void_fraction', 'void_volume [cm^3/g]', 'functional_groups', 'metal_linker', 'organic_linker1', 'organic_linker2', 'catalog CO2/N2', 'CO2/N2_selectivity', 'heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]']\n"
     ]
    }
   ],
   "source": [
    "feat = list(df_num.columns)[10:]\n",
    "feat.remove('surface_area [m^2/g]')\n",
    "feat.remove('CO2_working_capacity [mL/g]')\n",
    "print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8124e2d-3a79-4864-880b-3ff1fbaf7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in feat:\n",
    "    metadata_dict[col] = df[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "380ce521-b417-4ab9-a8e1-28b68362cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_embedding(y, metadata=list(zip(*metadata_dict.values())),\n",
    "              metadata_header=metadata_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05d99e89-8bf3-40dd-87ce-7a9b03cd5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3354c35a-8f4d-442a-b863-fc69ccb2d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "error\n",
      "surface area\n",
      "topology\n",
      "volume [A^3]\n",
      "weight [u]\n",
      "density [g/cm^3]\n",
      "void_fraction\n",
      "void_volume [cm^3/g]\n",
      "functional_groups\n",
      "metal_linker\n",
      "organic_linker1\n",
      "organic_linker2\n",
      "catalog CO2/N2\n",
      "CO2/N2_selectivity\n",
      "heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]\n"
     ]
    }
   ],
   "source": [
    "for i in metadata_dict:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd23390-9b8e-4f32-ae45-ac8eed31854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "X-ray_CUDA",
   "language": "python",
   "name": "x-ray_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
