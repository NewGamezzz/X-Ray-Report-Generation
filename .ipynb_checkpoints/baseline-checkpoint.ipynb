{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a5d559-ddc9-4854-8990-f1a7c71aa395",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d85df8b-5406-440c-80d5-c0277c1a3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchsummary import summary\n",
    "from tensorboard.plugins import projector\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307cdcd-66a9-4ce5-8402-26bfdcb0ccbf",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba50fba2-453d-4b20-8995-c607bee497fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, nheads=4, ## According to feature vectors that we get from SBERT, hidden_dim = 384\n",
    "                 num_encoder_layers=3, num_decoder_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create encoder and decoder layers\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        \n",
    "        # create a default PyTorch transformer: nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (sentence)\n",
    "        self.sentence = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings (may be changed to sin positional encodings)\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.backbone(X)\n",
    "        feat = self.conv(X)\n",
    "        H, W = feat.shape[-2:]\n",
    "        \n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        \n",
    "        feat = self.transformer_encoder(pos + 0.1 * feat.flatten(2).permute(2, 0, 1))\n",
    "        R = self.transformer_decoder(self.sentence.unsqueeze(1), feat).transpose(0, 1)\n",
    "        return R, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8b49b1-1a86-4d2c-8793-e50cf4f8efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 975, 800])\n"
     ]
    }
   ],
   "source": [
    "img_path = 'data/images'\n",
    "filenames = os.listdir(img_path)\n",
    "f = os.path.join(img_path, filenames[0])\n",
    "\n",
    "img = Image.open(f)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "t_img = transform_img(img, transform)\n",
    "print(t_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f20b0cc-c977-42cb-bdb1-0d9f62d12a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 384]) torch.Size([775, 1, 384])\n"
     ]
    }
   ],
   "source": [
    "model = Baseline()\n",
    "R, feat = model(t_img)\n",
    "\n",
    "print(R.shape, feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51135a-6009-4ae3-b548-6b2292134581",
   "metadata": {},
   "source": [
    "# LSP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb89e4b-0e32-434d-8621-80ee63c850a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSP_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, tgt, memory):\n",
    "        decode_sen = self.transformer_decoder(tgt, memory)\n",
    "        return decode_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13d223a-297a-4852-9126-02f89de20b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 384])\n"
     ]
    }
   ],
   "source": [
    "decoder = LSP_Decoder()\n",
    "\n",
    "N, c = 10, 384\n",
    "emb = nn.Embedding(N, c)\n",
    "x = torch.arange(N)\n",
    "x = emb(x).unsqueeze(1)\n",
    "\n",
    "y = decoder(x, feat)\n",
    "y = y.transpose(0, 1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e765fe-f4c0-48e0-a736-46a9e0b4dfec",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de10905-231e-4d7f-9e32-6c193d426ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "\n",
    "\n",
    "writer.add_embedding(y.reshape((10,384)))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b29a7ad-904b-4f35-b32b-3ce50e0d6e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6208), started 12:12:28 ago. (Use '!kill 6208' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34540d3dc53b9507\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34540d3dc53b9507\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e1fb2-83bc-46e6-a4ca-45e18ff9e635",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c8ec41-1658-40b2-b97d-a0c79524c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.array([[4, 1, 2], [2, 0, 5]])\n",
    "cost = cost.transpose(1, 0)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ab421f-af6d-4d3d-921c-e2838bf8c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "print(row_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eac125c-6029-4ff9-8fb3-4d3227d1f5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 0]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2901152-8e3a-45d4-b30f-d40b7d7ffcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "a = torch.where(a > 1, a, -a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f149eda7-78b8-48df-acff-f1d450b769c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_reject_GCR(vec, reject_vec, safe_conf=0.001):\n",
    "    \"\"\"\n",
    "    Remove reject vector from vec only if they form obtuse angle and fars.\n",
    "    Args:\n",
    "        vec: (n, d)\n",
    "        reject_vec: (n, d)\n",
    "    Returns:\n",
    "        vec: (n, d)\n",
    "    \"\"\"\n",
    "    norm_rej = F.normalize(reject_vec, dim=-1) ## unit vector\n",
    "    prod = (vec * norm_rej).sum(dim=-1, keepdim=True) ## dot product\n",
    "    # if obtuse => reject the gradient \n",
    "    proj_vec = norm_rej * torch.where(prod < 0, prod, torch.zeros_like(prod))\n",
    "    \n",
    "    safe_radius = vec.norm(dim=-1, keepdim=True) * safe_conf\n",
    "    rej_norm = reject_vec.norm(dim=-1, keepdim=True)\n",
    "    # if within safe_radius, don't reject\n",
    "    proj_vec = torch.where(rej_norm < safe_radius, torch.zeros_like(proj_vec), proj_vec)\n",
    "    \n",
    "    vec = vec - proj_vec\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e31a20f0-e2f1-43e8-bb4d-c67822901b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1102e-16, -1.1102e-16, -1.1102e-16],\n",
      "        [ 3.0000e+00,  4.0000e+00,  5.0000e+00],\n",
      "        [ 6.0000e+00,  7.0000e+00,  8.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "vec = torch.tensor([[-1, -1, -1], [3, 4, 5], [6, 7, 8]], dtype=float)\n",
    "rej_vec = torch.tensor([[100, 100, 100], [3, 4, 5], [6, 7, 8]], dtype=float)\n",
    "a = vector_reject_GCR(vec, rej_vec)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd89c5c-8f83-4f51-8acc-5b3b2b3527c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
