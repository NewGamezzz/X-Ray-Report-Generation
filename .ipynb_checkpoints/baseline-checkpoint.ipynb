{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a5d559-ddc9-4854-8990-f1a7c71aa395",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d85df8b-5406-440c-80d5-c0277c1a3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchsummary import summary\n",
    "from tensorboard.plugins import projector\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307cdcd-66a9-4ce5-8402-26bfdcb0ccbf",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba50fba2-453d-4b20-8995-c607bee497fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, nheads=4, ## According to feature vectors that we get from SBERT, hidden_dim = 384\n",
    "                 num_encoder_layers=3, num_decoder_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create encoder and decoder layers\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        \n",
    "        # create a default PyTorch transformer: nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (sentence)\n",
    "        self.sentence = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings (may be changed to sin positional encodings)\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.backbone(X)\n",
    "        feat = self.conv(X)\n",
    "        H, W = feat.shape[-2:]\n",
    "        \n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        \n",
    "        feat = self.transformer_encoder(pos + 0.1 * feat.flatten(2).permute(2, 0, 1))\n",
    "        R = self.transformer_decoder(self.sentence.unsqueeze(1), feat).transpose(0, 1)\n",
    "        return R, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8b49b1-1a86-4d2c-8793-e50cf4f8efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 975, 800])\n"
     ]
    }
   ],
   "source": [
    "img_path = 'data/images'\n",
    "filenames = os.listdir(img_path)\n",
    "f = os.path.join(img_path, filenames[0])\n",
    "\n",
    "img = Image.open(f)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "t_img = transform_img(img, transform)\n",
    "print(t_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f20b0cc-c977-42cb-bdb1-0d9f62d12a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 384]) torch.Size([775, 1, 384])\n"
     ]
    }
   ],
   "source": [
    "model = Baseline()\n",
    "R, feat = model(t_img)\n",
    "\n",
    "print(R.shape, feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51135a-6009-4ae3-b548-6b2292134581",
   "metadata": {},
   "source": [
    "# LSP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb89e4b-0e32-434d-8621-80ee63c850a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSP_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, tgt, memory):\n",
    "        decode_sen = self.transformer_decoder(tgt, memory)\n",
    "        return decode_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d13d223a-297a-4852-9126-02f89de20b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 384])\n"
     ]
    }
   ],
   "source": [
    "decoder = LSP_Decoder()\n",
    "\n",
    "N, c = 10, 384\n",
    "emb = nn.Embedding(N, c)\n",
    "x = torch.arange(N)\n",
    "x = emb(x).unsqueeze(1)\n",
    "\n",
    "y = decoder(x, feat)\n",
    "y = y.transpose(0, 1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e765fe-f4c0-48e0-a736-46a9e0b4dfec",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4de10905-231e-4d7f-9e32-6c193d426ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "\n",
    "\n",
    "writer.add_embedding(y.reshape((10,384)))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b29a7ad-904b-4f35-b32b-3ce50e0d6e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6208), started 0:16:50 ago. (Use '!kill 6208' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a268c5a0ed5c2e7c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a268c5a0ed5c2e7c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e1fb2-83bc-46e6-a4ca-45e18ff9e635",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c8ec41-1658-40b2-b97d-a0c79524c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ab421f-af6d-4d3d-921c-e2838bf8c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(row_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac125c-6029-4ff9-8fb3-4d3227d1f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
