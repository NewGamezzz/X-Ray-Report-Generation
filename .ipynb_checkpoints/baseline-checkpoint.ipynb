{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a5d559-ddc9-4854-8990-f1a7c71aa395",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d85df8b-5406-440c-80d5-c0277c1a3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchsummary import summary\n",
    "from tensorboard.plugins import projector\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from utils.utils import *\n",
    "from utils.assignment import *\n",
    "from utils.latent_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ddddbb-cd5d-4278-a860-6925d85ce3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "    return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db3811-cec9-4dcc-b09d-29c094143dde",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4239d38-0cb1-4cfc-b670-500936f5365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, transform):\n",
    "        self.data = data\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_path).resize((256, 256))\n",
    "        label = self.data.iloc[idx, 2]\n",
    "        \n",
    "        image = transform_img(image, self.transform)\n",
    "        _, label = tokenize_report(label)\n",
    "        return image[0], len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0747947d-b132-4af5-8e18-bbace5203262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/training_set.csv')\n",
    "df_test = pd.read_csv('data/testing_set.csv')\n",
    "\n",
    "img_path = 'data/images'\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = XRayDataset(df_train, img_path, transform)\n",
    "test_data = XRayDataset(df_test, img_path, transform)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307cdcd-66a9-4ce5-8402-26bfdcb0ccbf",
   "metadata": {},
   "source": [
    "# CNN Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439c8c37-ff90-4a4e-b533-ecd5926626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Feat(nn.Module):\n",
    "    def __init__(self, hidden_dim=348):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        feat = self.backbone(X)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba50fba2-453d-4b20-8995-c607bee497fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, nheads=4, ## According to feature vectors that we get from SBERT, hidden_dim = 384\n",
    "                 num_encoder_layers=3, num_decoder_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.conv_feat = CNN_Feat(hidden_dim)\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create encoder and decoder layers\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        \n",
    "        # create a default PyTorch transformer: nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (sentence)\n",
    "        self.sentence = nn.Parameter(torch.rand(10, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings (may be changed to sin positional encodings)\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        feat = self.conv_feat(X)\n",
    "        feat = self.conv(feat)\n",
    "        H, W = feat.shape[-2:]\n",
    "        \n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        \n",
    "        feat = self.transformer_encoder(pos + 0.1 * feat.flatten(2).permute(2, 0, 1))\n",
    "        R = self.transformer_decoder(self.sentence.unsqueeze(1), feat).transpose(0, 1)\n",
    "        return R, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8b49b1-1a86-4d2c-8793-e50cf4f8efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 975, 800])\n"
     ]
    }
   ],
   "source": [
    "img_path = 'data/images'\n",
    "filenames = os.listdir(img_path)\n",
    "f = os.path.join(img_path, filenames[0])\n",
    "\n",
    "img = Image.open(f)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "t_img = transform_img(img, transform)\n",
    "print(t_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f20b0cc-c977-42cb-bdb1-0d9f62d12a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 384]) torch.Size([775, 1, 384])\n"
     ]
    }
   ],
   "source": [
    "model = CNN_Text()\n",
    "R, feat = model(t_img)\n",
    "\n",
    "print(R.shape, feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51135a-6009-4ae3-b548-6b2292134581",
   "metadata": {},
   "source": [
    "# LSP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb89e4b-0e32-434d-8621-80ee63c850a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSP_Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=384, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers=num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt, memory):\n",
    "        decode_sen = self.transformer_decoder(tgt, memory)\n",
    "        decode_sen = F.softmax(self.linear(decode_sen), dim=-1)\n",
    "        return decode_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13d223a-297a-4852-9126-02f89de20b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 384])\n",
      "torch.Size([1, 10, 2000])\n"
     ]
    }
   ],
   "source": [
    "decoder = LSP_Decoder(vocab_size = 2000)\n",
    "\n",
    "N, c = 10, 384\n",
    "emb = nn.Embedding(N, c)\n",
    "x = torch.arange(N)\n",
    "x = emb(x).unsqueeze(1)\n",
    "print(x.shape)\n",
    "\n",
    "y = decoder(x, feat)\n",
    "y = y.transpose(0, 1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e1fb2-83bc-46e6-a4ca-45e18ff9e635",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c1fa8a-0acd-4f9f-9c3e-5444eca18193",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSEGCRLatentLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2716e2a1-cf92-4583-8961-15dbc833a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8250, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B = torch.tensor([[2, 0], [1, 2], [5, 3]], dtype=float, requires_grad=True)\n",
    "len_B = torch.tensor([1, 2])\n",
    "\n",
    "R = torch.tensor([[2, 0], [2, 0], [5, 2], [10, 2]], dtype=float, requires_grad=True)\n",
    "len_R = torch.tensor([1, 3])\n",
    "\n",
    "R_pi, R_i, loss = loss_fn.forward(B, len_B, R, len_R)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "990f50fe-2d05-47ea-a316-fa850a80b681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[2., 0.],\n",
       "         [5., 2.]]], dtype=torch.float64, grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pad_by_lengths(R_pi, len_B, batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49b7c6-1c1c-484a-96c7-026d746703bc",
   "metadata": {},
   "source": [
    "# MLP Predict lenght of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae56db63-b555-4c7c-8ed7-0515f304aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=384):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_feat = CNN_Feat(hidden_dim)\n",
    "        self.linear1 = nn.Linear(131072, 32)\n",
    "        self.linear2 = nn.Linear(32, 32)\n",
    "        self.linear3 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        feat = self.conv_feat(X)\n",
    "        x = torch.flatten(feat, 1)\n",
    "        x = self.linear1(x).relu()\n",
    "        x = self.linear2(x).relu()\n",
    "        x = self.linear3(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        output = self.output(x).relu()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d95be59-e958-4458-929d-3b0186370b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    c=0\n",
    "    correct=0\n",
    "    for X, y in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(X.to(device))\n",
    "    \n",
    "        loss = criterion(torch.flatten(out), y.to(device))  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        c=c+1\n",
    "        correct+=loss.cpu().detach().numpy()\n",
    "    return correct/c\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    c=0\n",
    "    for X, y in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(X.to(device))\n",
    "    \n",
    "        loss = criterion(torch.flatten(out), y.to(device)) # Compute the loss.\n",
    "        correct += loss.cpu().detach().numpy()  # Check against ground-truth labels.\n",
    "        c=c+1\n",
    "    return correct / c  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ae275a-0a44-493a-bb4e-bf32f48c769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "\t linear3.weight\n",
      "\t linear3.bias\n",
      "\t output.weight\n",
      "\t output.bias\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "params_to_update = model.parameters()\n",
    "\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "        \n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.001)\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c6a0a-7688-43f0-8ed8-fb31cbcd61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "epochs = 25\n",
    "min_loss = 1.54\n",
    "print('start train')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_loss.append(train_acc)\n",
    "    test_loss.append(test_acc)\n",
    "    scheduler.step(test_acc)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train MAE: {train_acc:.4f}, Test MAE: {test_acc:.4f}')\n",
    "    if min_loss > test_acc:\n",
    "        min_loss = test_acc\n",
    "        print('Minimum Loss: {}'.format(min_loss))\n",
    "        torch.save(model.state_dict(), \"model/length_model_best2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b7113-df7f-40d3-99bc-be6c7c3f5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('loss')\n",
    "plt.plot(np.arange(epochs), train_loss, label='train loss')\n",
    "plt.plot(np.arange(epochs), test_loss, label='val loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e765fe-f4c0-48e0-a736-46a9e0b4dfec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de10905-231e-4d7f-9e32-6c193d426ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "\n",
    "\n",
    "writer.add_embedding(y.reshape((-1, 4))) ##y.reshape((10,384))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29a7ad-904b-4f35-b32b-3ce50e0d6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "X-ray_CUDA",
   "language": "python",
   "name": "x-ray_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
